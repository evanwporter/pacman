{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f800fabc",
      "metadata": {
        "id": "f800fabc"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bdd597",
      "metadata": {
        "id": "56bdd597"
      },
      "source": [
        "* State: What agent sees\n",
        "* Action: What agent can do\n",
        "* Rewards: Feedback after the agent does something\n",
        "* Q-value: A number that says how good an action is"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224b06b7",
      "metadata": {
        "id": "224b06b7"
      },
      "source": [
        "## Rewards\n",
        "\n",
        "* Pac-Dot = 10 Pts\n",
        "* Power Pellet = 50 Pts\n",
        "* 1st Ghost = 200 Pts\n",
        "* 2nd Ghost = 400 Pts\n",
        "* 3rd Ghost = 800 Pts\n",
        "* 4th Ghost = 1600 Pts\n",
        "* Cherry = 100 Pts\n",
        "* Strawberry = 300 Pts\n",
        "* Orange = 500 Pts\n",
        "* Apple = 700 Pts\n",
        "* Melon = 1000 Pts\n",
        "* Galaxian = 2000 Pts\n",
        "* Bell = 3000 Pts\n",
        "* Key = 5000 Pts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d5e14a1",
      "metadata": {
        "id": "7d5e14a1"
      },
      "outputs": [],
      "source": [
        "# At the beginning of each life there is a period of\n",
        "# time where the game doesn't allow the player to\n",
        "# move Pacman we skip those steps\n",
        "AVOIDED_STEPS = 80\n",
        "\n",
        "REWARD_LOG_BASE = 1000\n",
        "LOSE_REWARD = -3\n",
        "WIN_REWARD = 3\n",
        "\n",
        "# Observation size\n",
        "OBS_SIZE = 128\n",
        "\n",
        "# Actions\n",
        "ACTIONS = [\n",
        "    1,  # UP\n",
        "    2,  # RIGHT\n",
        "    3,  # LEFT\n",
        "    4,  # DOWN\n",
        "]\n",
        "N_ACTIONS = 4\n",
        "\n",
        "EPSILON_MAX = 1.0\n",
        "EPSILON_MIN = 0.1\n",
        "EPSILON_DECAY = 1_000_000\n",
        "\n",
        "# How often to update the target model so its the same\n",
        "# as the policy model (in steps)\n",
        "TARGET_UPDATE_FREQ = 8_000\n",
        "\n",
        "# How many elements to pull from the replay buffer\n",
        "# at once to use for training\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Device to run on\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# How often should we save the model (in episodes)\n",
        "SAVE_MODEL = 20\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "# How much future rewards are valued compared to current rewards\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# How many frames to skip\n",
        "SKIP_FRAMES = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b8865e",
      "metadata": {
        "id": "68b8865e"
      },
      "source": [
        "We log the reward scale so that large rewards (such as eating a ghost which can be up 1600 points) don't provide an outsized effect on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9bd2ab89",
      "metadata": {
        "id": "9bd2ab89"
      },
      "outputs": [],
      "source": [
        "def transform_reward(reward: int):\n",
        "    return np.emath.logn(REWARD_LOG_BASE, reward) if reward > 0 else reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4afee2e0",
      "metadata": {
        "id": "4afee2e0"
      },
      "outputs": [],
      "source": [
        "Experience = namedtuple(\n",
        "    \"Experience\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1f1525e4",
      "metadata": {
        "id": "1f1525e4"
      },
      "outputs": [],
      "source": [
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32, device=DEVICE)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64, device=DEVICE)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
        "        next_states = torch.tensor(\n",
        "            np.array(next_states), dtype=torch.float32, device=DEVICE\n",
        "        )\n",
        "        dones = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2f7ce8be",
      "metadata": {
        "id": "2f7ce8be"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(OBS_SIZE, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, N_ACTIONS)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "43d7949d",
      "metadata": {
        "id": "43d7949d"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataRecord:\n",
        "    rewards: np.ndarray = field(\n",
        "        default_factory=lambda: np.zeros(NUM_EPISODES, dtype=np.int64)\n",
        "    )\n",
        "    qvalues: np.ndarray = field(\n",
        "        default_factory=lambda: np.zeros(NUM_EPISODES, dtype=np.float64)\n",
        "    )\n",
        "    losses: np.ndarray = field(\n",
        "        default_factory=lambda: np.zeros(NUM_EPISODES, dtype=np.float64)\n",
        "    )\n",
        "    episodes: int = 0\n",
        "    successes: int = field(default_factory=int)\n",
        "\n",
        "    def __iter__(self):\n",
        "        yield self.losses, self.rewards, self.qvalues\n",
        "\n",
        "    def append(self, reward, qval, loss):\n",
        "        # we need to dynamically reallocate it similar to what\n",
        "        # std::vector does in C++\n",
        "        if self.episodes >= len(self.rewards):\n",
        "            # Double the size\n",
        "            new_size = len(self.rewards) * 2\n",
        "            self.rewards = np.resize(self.rewards, new_size)\n",
        "            self.qvalues = np.resize(self.qvalues, new_size)\n",
        "            self.losses = np.resize(self.losses, new_size)\n",
        "\n",
        "        self.rewards[self.episodes] = reward\n",
        "        self.qvalues[self.episodes] = qval\n",
        "        self.losses[self.episodes] = loss\n",
        "\n",
        "        self.episodes += 1\n",
        "\n",
        "    def to_pandas(self):\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"Rewards\": self.rewards[: self.episodes],\n",
        "                \"Q-Values\": self.qvalues[: self.episodes],\n",
        "                \"Losses\": self.losses[: self.episodes],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def plot(self, save_path=None):\n",
        "        episodes = range(self.episodes)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Plot Rewards\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(episodes, self.rewards[: self.episodes], label=\"Reward\")\n",
        "        plt.title(\"Episode Rewards\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Q-Values\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(\n",
        "            episodes, self.qvalues[: self.episodes], label=\"Q-Value\", color=\"orange\"\n",
        "        )\n",
        "        plt.title(\"Mean Q-Values\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Q-Value\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Losses\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(episodes, self.losses[: self.episodes], label=\"Loss\", color=\"red\")\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            print(f\"Plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "E0lRfanzbq6L",
      "metadata": {
        "id": "E0lRfanzbq6L"
      },
      "outputs": [],
      "source": [
        "class MsPacman:\n",
        "    def __init__(self):\n",
        "        self.steps_done = 0\n",
        "\n",
        "        self.policy_net = DQN().to(DEVICE)\n",
        "        self.target_net = DQN().to(DEVICE)\n",
        "\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=10_000)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Given a state choose an action (random or best possible)\n",
        "        eps_threshold = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * np.exp(\n",
        "            -1.0 * self.steps_done / EPSILON_DECAY\n",
        "        )\n",
        "\n",
        "        self.steps_done += 1\n",
        "\n",
        "        # EXPLORE\n",
        "        if random.random() < eps_threshold:\n",
        "            # Generate a single random integer between 1 (inclusive) and 5 (exclusive)\n",
        "            return np.random.randint(4), None\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(\n",
        "                    state, dtype=torch.float32, device=DEVICE\n",
        "                ).unsqueeze(0)\n",
        "                q_values = self.policy_net(state)\n",
        "                return q_values.argmax().item(), q_values.max(1)[0].item()\n",
        "\n",
        "    def memorize(self, experience):\n",
        "        self.memory.push(experience)\n",
        "\n",
        "    def recall(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)\n",
        "        actions = actions.unsqueeze(1)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.recall()\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            best_actions = self.policy_net(next_states).argmax(1)\n",
        "            next_q_values = (\n",
        "                self.target_net(next_states)\n",
        "                .gather(1, best_actions.unsqueeze(1))\n",
        "                .squeeze(1)\n",
        "            )\n",
        "            target_q = rewards + DISCOUNT * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_values, target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def sync(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "vnMYHENWe06j",
      "metadata": {
        "id": "vnMYHENWe06j"
      },
      "outputs": [],
      "source": [
        "class Scheduler:\n",
        "    def __init__(self, render: bool = True):\n",
        "        self.episodes = 0\n",
        "        self.steps_done = 0\n",
        "\n",
        "        render_mode = \"human\" if render else None\n",
        "        self.env = gym.make(\"ALE/MsPacman-v5\", render_mode=render_mode, obs_type=\"ram\")\n",
        "\n",
        "        self.record = DataRecord()\n",
        "\n",
        "        self.actor = MsPacman()\n",
        "\n",
        "    def run_one_episode(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        lives = 3\n",
        "        total_reward = 0\n",
        "\n",
        "        total_qval = 0.0\n",
        "        total_loss = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        self.episodes += 1\n",
        "\n",
        "        # Skip initial frames\n",
        "        for _ in range(AVOIDED_STEPS):\n",
        "            self.env.step(3)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action_index, q_val = self.actor.act(obs)\n",
        "            action = ACTIONS[action_index]\n",
        "\n",
        "            total_step_reward = 0\n",
        "\n",
        "            for _ in range(SKIP_FRAMES):\n",
        "                next_obs, reward_, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "                if reward_ in {200, 400, 800, 1600}:\n",
        "                    print(\"Ate a ghost\")\n",
        "\n",
        "                reward = transform_reward(reward_)\n",
        "\n",
        "                # We ran into a ghost\n",
        "                if info[\"lives\"] < lives:\n",
        "                    lives -= 1\n",
        "                    reward += LOSE_REWARD\n",
        "\n",
        "                if terminated and lives > 0:\n",
        "                    reward += WIN_REWARD\n",
        "\n",
        "                total_step_reward += reward\n",
        "                done = terminated or truncated\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward += total_step_reward\n",
        "\n",
        "            self.actor.memorize((obs, action_index, total_step_reward, next_obs, done))\n",
        "            loss = self.actor.learn()\n",
        "\n",
        "            # Record Q-value and loss for this step\n",
        "            if q_val is not None:\n",
        "                total_qval += q_val\n",
        "            if loss is not None:\n",
        "                total_loss += loss\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            if self.steps_done % TARGET_UPDATE_FREQ == 0:\n",
        "                self.actor.sync()\n",
        "\n",
        "            step_count += 1\n",
        "            self.steps_done += 1\n",
        "\n",
        "        mean_qval = total_qval / step_count if step_count > 0 else 0.0\n",
        "        mean_loss = total_loss / step_count if step_count > 0 else 0.0\n",
        "        self.record.append(total_reward, mean_qval, mean_loss)\n",
        "\n",
        "        print(f\"[Episode {self.episodes}] Total Reward: {total_reward:.3f}\")\n",
        "\n",
        "    def run(self, num_episodes=1000):\n",
        "        for _ in range(num_episodes):\n",
        "            self.run_one_episode()\n",
        "\n",
        "            if self.episodes % SAVE_MODEL == 0 and self.episodes != 0:\n",
        "                self.save_checkpoint(\"./results/checkpoint.pth\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        checkpoint = {\n",
        "            \"episode\": self.episodes,\n",
        "            \"steps_done\": self.actor.steps_done,\n",
        "            \"model_state_dict\": self.actor.policy_net.state_dict(),\n",
        "            \"target_state_dict\": self.actor.target_net.state_dict(),\n",
        "            \"optimizer_state_dict\": self.actor.optimizer.state_dict(),\n",
        "            \"replay_buffer\": list(self.actor.memory.buffer),  # deque to list\n",
        "            \"record\": {\n",
        "                \"rewards\": self.record.rewards,\n",
        "                \"qvalues\": self.record.qvalues,\n",
        "                \"losses\": self.record.losses,\n",
        "                \"episodes\": self.record.episodes,\n",
        "                \"successes\": self.record.successes,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "        print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, path=\"checkpoint.pth\"):\n",
        "        checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)\n",
        "        self.episodes = checkpoint[\"episode\"]\n",
        "        self.actor.steps_done = checkpoint[\"steps_done\"]\n",
        "\n",
        "        self.actor.policy_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.actor.target_net.load_state_dict(checkpoint[\"target_state_dict\"])\n",
        "        self.actor.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        self.actor.memory.buffer = deque(checkpoint[\"replay_buffer\"], maxlen=10_000)\n",
        "\n",
        "        self.record.rewards = checkpoint[\"record\"][\"rewards\"]\n",
        "        self.record.qvalues = checkpoint[\"record\"][\"qvalues\"]\n",
        "        self.record.losses = checkpoint[\"record\"][\"losses\"]\n",
        "        self.record.episodes = checkpoint[\"record\"][\"episodes\"]\n",
        "        self.record.successes = checkpoint[\"record\"][\"successes\"]\n",
        "\n",
        "        print(f\"Checkpoint loaded from {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "88b7f644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88b7f644",
        "outputId": "848ffd43-d7bc-429e-a8d7-dac5b24bb576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint loaded from ./results/checkpoint.pth\n",
            "Ate a ghost\n",
            "Ate a ghost\n",
            "Ate a ghost\n",
            "[Episode 3961] Total Reward: 22.867\n",
            "[Episode 3962] Total Reward: 14.566\n",
            "Ate a ghost\n",
            "[Episode 3963] Total Reward: 9.333\n",
            "Ate a ghost\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo checkpoint found. Starting fresh.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m scheduler.run(NUM_EPISODES)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mScheduler.run\u001b[39m\u001b[34m(self, num_episodes)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_episodes=\u001b[32m1000\u001b[39m):\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         \u001b[38;5;28mself\u001b[39m.run_one_episode()\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.episodes % SAVE_MODEL == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.episodes != \u001b[32m0\u001b[39m:\n\u001b[32m     88\u001b[39m             \u001b[38;5;28mself\u001b[39m.save_checkpoint(\u001b[33m\"\u001b[39m\u001b[33m./results/checkpoint.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mScheduler.run_one_episode\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m total_reward += total_step_reward\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.actor.memorize((obs, action_index, total_step_reward, next_obs, done))\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.actor.learn()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Record Q-value and loss for this step\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m q_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mMsPacman.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m     64\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     adam(\n\u001b[32m    245\u001b[39m         params_with_grad,\n\u001b[32m    246\u001b[39m         grads,\n\u001b[32m    247\u001b[39m         exp_avgs,\n\u001b[32m    248\u001b[39m         exp_avg_sqs,\n\u001b[32m    249\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    250\u001b[39m         state_steps,\n\u001b[32m    251\u001b[39m         amsgrad=group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    252\u001b[39m         has_complex=has_complex,\n\u001b[32m    253\u001b[39m         beta1=beta1,\n\u001b[32m    254\u001b[39m         beta2=beta2,\n\u001b[32m    255\u001b[39m         lr=group[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    256\u001b[39m         weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    257\u001b[39m         eps=group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    258\u001b[39m         maximize=group[\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    259\u001b[39m         foreach=group[\u001b[33m\"\u001b[39m\u001b[33mforeach\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    260\u001b[39m         capturable=group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    261\u001b[39m         differentiable=group[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    262\u001b[39m         fused=group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    263\u001b[39m         grad_scale=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrad_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    264\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    265\u001b[39m     )\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m func(\n\u001b[32m    877\u001b[39m     params,\n\u001b[32m    878\u001b[39m     grads,\n\u001b[32m    879\u001b[39m     exp_avgs,\n\u001b[32m    880\u001b[39m     exp_avg_sqs,\n\u001b[32m    881\u001b[39m     max_exp_avg_sqs,\n\u001b[32m    882\u001b[39m     state_steps,\n\u001b[32m    883\u001b[39m     amsgrad=amsgrad,\n\u001b[32m    884\u001b[39m     has_complex=has_complex,\n\u001b[32m    885\u001b[39m     beta1=beta1,\n\u001b[32m    886\u001b[39m     beta2=beta2,\n\u001b[32m    887\u001b[39m     lr=lr,\n\u001b[32m    888\u001b[39m     weight_decay=weight_decay,\n\u001b[32m    889\u001b[39m     eps=eps,\n\u001b[32m    890\u001b[39m     maximize=maximize,\n\u001b[32m    891\u001b[39m     capturable=capturable,\n\u001b[32m    892\u001b[39m     differentiable=differentiable,\n\u001b[32m    893\u001b[39m     grad_scale=grad_scale,\n\u001b[32m    894\u001b[39m     found_inf=found_inf,\n\u001b[32m    895\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evanw\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:400\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    398\u001b[39m     grad = grad.add(param, alpha=weight_decay)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_complex(param):\n\u001b[32m    401\u001b[39m     grad = torch.view_as_real(grad)\n\u001b[32m    402\u001b[39m     exp_avg = torch.view_as_real(exp_avg)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "scheduler = Scheduler(render=False)\n",
        "\n",
        "try:\n",
        "    scheduler.load_checkpoint(\"./results/checkpoint.pth\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No checkpoint found. Starting fresh.\")\n",
        "\n",
        "scheduler.run(NUM_EPISODES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa0d611",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "daa0d611",
        "outputId": "c3a1eb9f-0b76-44c0-a333-332ae1e9c89a"
      },
      "outputs": [],
      "source": [
        "scheduler.record.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DrWBy_KKBu79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "DrWBy_KKBu79",
        "outputId": "02f2d033-847e-4fdf-e2f0-374fd9451283"
      },
      "outputs": [],
      "source": [
        "window = 20\n",
        "df = scheduler.record.to_pandas()\n",
        "df[\"RollingReward\"] = df[\"Rewards\"].rolling(window).mean()\n",
        "df[\"RollingQ\"] = df[\"Q-Values\"].rolling(window).mean()\n",
        "\n",
        "# Plot both\n",
        "plt.figure(figsize=(9, 4))\n",
        "\n",
        "# Plot rewards and rolling average\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(df[\"Rewards\"], label=\"Reward\")\n",
        "plt.plot(df[\"RollingReward\"], label=f\"Rolling Avg ({window})\", linewidth=2)\n",
        "plt.title(\"Episode Reward with Rolling Average\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot Q-values and rolling average\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(df[\"Q-Values\"], label=\"Q-Value\", color=\"orange\")\n",
        "plt.plot(df[\"RollingQ\"], label=f\"Rolling Avg ({window})\", color=\"red\", linewidth=2)\n",
        "plt.title(\"Mean Q-Value with Rolling Average\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Q-Value\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "P3Vqd8qn8i7C",
      "metadata": {
        "id": "P3Vqd8qn8i7C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint loaded from ./results/checkpoint.pth\n"
          ]
        }
      ],
      "source": [
        "scheduler = Scheduler(render=True)\n",
        "\n",
        "try:\n",
        "    scheduler.load_checkpoint(\"./results/checkpoint.pth\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No checkpoint found. Starting fresh.\")\n",
        "\n",
        "obs, _ = scheduler.env.reset()\n",
        "lives = 3\n",
        "total_reward = 0\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    with torch.no_grad():\n",
        "        state = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "        q_values = scheduler.actor.target_net(state)\n",
        "        action_index = q_values.argmax().item()\n",
        "\n",
        "    action = ACTIONS[action_index]\n",
        "    obs, reward_, terminated, truncated, info = scheduler.env.step(action)\n",
        "\n",
        "    done = terminated or truncated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c31315",
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def record_gif(env, policy_net, save_path=\"pacman_run.gif\", max_steps=1000):\n",
        "    frames = []\n",
        "    obs, _ = env.reset()\n",
        "    obs = obs / 255.0\n",
        "\n",
        "    done = False\n",
        "    lives = 3\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        frame = env.render()  # RGB array (H, W, 3)\n",
        "        frames.append(Image.fromarray(frame))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "            q_values = policy_net(state)\n",
        "            action_index = q_values.argmax().item()\n",
        "\n",
        "        action = ACTIONS[action_index]\n",
        "\n",
        "        next_obs, reward_, terminated, truncated, info = env.step(action)\n",
        "        next_obs = next_obs / 255.0\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "        steps += 1\n",
        "\n",
        "    # Save to GIF\n",
        "    frames[0].save(\n",
        "        save_path,\n",
        "        save_all=True,\n",
        "        append_images=frames[1:],\n",
        "        duration=40,  # ms per frame (~25 FPS)\n",
        "        loop=0,\n",
        "    )\n",
        "    print(f\"🎥 Saved gameplay to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e42e6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎥 Saved gameplay to pacman.gif\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\", obs_type=\"ram\")\n",
        "model = scheduler.actor.target_net\n",
        "\n",
        "record_gif(env, model, save_path=\"pacman.gif\")\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
